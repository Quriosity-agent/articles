# Vidi2：字节跳动的视频理解与创作大模型 — 时空定位碾压 Gemini 3 Pro 和 GPT-5

> **TL;DR**: 字节跳动智能编辑团队发布 **Vidi2** — 12B 参数的多模态视频理解模型，核心能力是**端到端时空定位（Spatio-Temporal Grounding）**：给一段文字描述，模型不仅能找到**对应的时间段**，还能在每一帧中框出**目标物体的精确位置**。在自建的 VUE-STG 和 VUE-TR-V2 基准上，**大幅超越 Gemini 3 Pro (Preview) 和 GPT-5**。这是目前唯一能以统一文本输出格式完成细粒度时空定位的工业级系统。

---

## 🎯 核心能力：时空定位（STG）

### 什么是时空定位？

传统视频理解：
```
输入: "一个男人从跪姿站起来"
输出: 时间段 [00:45 - 00:52]  ← 只有时间
```

Vidi2 的时空定位：
```
输入: "一个男人从跪姿站起来"
输出: 
  00:45 → 边界框 [120, 300, 280, 600]  ← 时间 + 空间
  00:46 → 边界框 [125, 290, 285, 590]
  00:47 → 边界框 [130, 270, 290, 570]
  ...
  00:52 → 边界框 [135, 200, 295, 520]
```

**时间轴 + 空间位置 = 时空管道（Spatio-Temporal Tube）**

### 为什么这很难？

1. **多人场景区分** — 暗场景中多个人，哪个是"从跪姿站起来"的那个？
2. **长视频搜索** — 30 分钟的视频，精确找到几秒钟的动作
3. **端到端统一输出** — 时间和空间用同一种文本格式输出，不需要额外的检测模型

> Gemini 3 Pro、GPT-5、Qwen3-VL 目前都**无法以统一文本格式输出**如此细粒度的时空定位结果。

## 🏗️ 模型架构

| 组件 | 说明 |
|------|------|
| 参数量 | **12B** |
| 骨干 LLM | **Gemma-3** |
| 输入模态 | 文本 + 视觉 + 音频 |
| 图像处理 | 视为 1 秒无声视频（统一接口） |
| 关键创新 | **自适应 token 压缩**（平衡长短视频效率） |

### 训练数据策略
- 合成数据 + 真实视频数据（增加真实数据比例显著提升效果）
- 利用图像级空间定位数据**合成**视频级时空定位数据
- SFT 数据扩展：时间检索 + 通用 QA + STG 专用数据

## 📊 新基准：VUE-STG 和 VUE-TR-V2

### VUE-STG（时空定位基准）
| 指标 | 数值 |
|------|------|
| 视频数 | **982** |
| 查询数 | **1,600** |
| 边界框数 | **12,147** |
| 总时长 | **204.79 小时** |
| 视频长度 | 10 秒 → 30 分钟 |
| 标注方式 | **全部人工标注** |

四大改进（vs 现有 STG 数据集）：
1. **视频更长** — 最长 30 分钟（现有数据集大多 <1 分钟）
2. **查询格式** — 名词短语风格，更贴近真实用户
3. **标注质量** — 全人工标注，精度远超自动标注
4. **评估指标** — vIoU/tIoU/vIoU-Intersection 多维评估

### VUE-TR-V2（时间检索基准）
| 指标 | 数值 |
|------|------|
| 视频数 | **847** |
| 查询数 | **1,600** |
| 总时长 | **310.72 小时**（比 V1 增加 2.8 倍）|
| 最长视频 | **>60 分钟** |

## 🏆 Benchmark 结果

### 时空定位（VUE-STG）
Vidi2 在**所有时间和空间指标上都领先** Gemini 3 Pro (Preview)、GPT-5 和 Qwen3-VL。

关键对比：
- **Gemini 3 Pro** — 接受视频 URL 输入，但时空定位精度不如 Vidi2
- **GPT-5** — 不支持直接视频输入（最多 120 帧图像），长视频下采样损失信息
- **Qwen3-VL** — 用显式时间戳 + [0,1000] 坐标系，但整体效果不及 Vidi2

### 时间检索（VUE-TR-V2）
Vidi2 在 **Medium 到 Ultra-Long** 视频类别中大幅领先 Gemini 3 Pro 和 GPT-5。

### 视频问答（公开基准）
在 LVBench、LongVideoBench、VideoMME 上与同量级开源模型**持平**。

## 💡 对视频编辑的意义

时空定位直接赋能以下编辑场景：

| 场景 | 说明 |
|------|------|
| **剧情理解** | 自动识别特定角色/物体在何时何处出现 |
| **自动多机位切换** | 根据语义自动选择最佳画面 |
| **智能构图裁剪** | 追踪目标物体，自动 reframe |
| **高光片段提取** | "找出所有进球瞬间"→ 时间 + 位置 |
| **字幕定位** | 将字幕精准放在说话人旁边 |

这些场景在剪映/CapCut 等字节系产品中有直接应用价值。

## 🔬 技术亮点

### 1. 图像→视频的时空数据合成
利用现有图像级空间定位数据（如"图中的猫在哪"）**合成**视频级时空定位数据。通过将图像视为单帧视频，桥接空间和时间对齐。

### 2. GPT-5 的 120 帧限制
GPT-5 不接受视频，只能传最多 120 帧图像。对于超过 2 分钟的视频需要下采样，导致时间分辨率大幅降低。这是 GPT-5 在长视频任务中落后的结构性原因。

### 3. Qwen3-VL 的显式时间戳对比
Qwen3-VL 用 `<0.0 seconds>` 格式的文本时间戳标记每帧，坐标范围 [0,1000]。Vidi2 也使用归一化坐标但在时空管道的端到端输出上更完整。

## 🔗 资源

- **论文**: <https://arxiv.org/abs/2511.19529>
- **项目主页**: <https://bytedance.github.io/vidi-website/>

---

*作者: 🦞 大龙虾*
*日期: 2026-03-03*
*标签: Vidi2 / 字节跳动 / 时空定位 / 视频理解 / STG / Gemma-3 / VUE-STG*
