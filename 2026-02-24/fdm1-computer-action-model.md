# 首个通用计算机操作模型：Standard Intelligence 发布 FDM-1

**Standard Intelligence 团队 | 2026年2月23日**

---

## 一句话总结

Standard Intelligence 在 1100 万小时的屏幕录制视频上训练了一个基础模型 FDM-1，能以 30 FPS 实时操作复杂网站、完成多步 CAD 建模，甚至在真实世界中驾驶汽车。

---

## 为什么这很重要？

在 FDM-1 之前，构建"电脑操作 AI"的标准做法是：用人工标注的截图微调视觉语言模型（VLM），再针对每个下游任务搭建强化学习环境。这种方法有三个致命瓶颈：

1. **上下文极短** — 只能处理几秒钟的画面
2. **数据集极小** — 最大的开源数据集不到 20 小时
3. **无法泛化** — 每个任务都要单独训练

与此同时，互联网上已经积累了数百万小时的影片剪辑直播、编程录屏和游戏实况。FDM-1 是第一个能在这个规模上训练的模型。

---

## 核心技术突破

### 1. 超高效视频编码器：100 倍压缩提升

传统 VLM 用 100 万 token 才能理解 1 分钟的 30 FPS 视频。FDM-1 的视频编码器用同样的 token 数可以编码**将近 2 小时**的视频——比前代 SOTA 高效 50 倍，比 OpenAI 的编码器高效 100 倍。

| 上下文窗口 | 可容纳视频时长 |
|---|---|
| 32k tokens | 3 分 30 秒 |
| 200k tokens | 20 分钟 |
| 1M tokens | 1 小时 40 分钟 |

关键创新在于：屏幕录制的信息密度变化极大（空白桌面 vs 密集代码页），团队通过**掩码压缩目标**的无监督训练，让编码器在高压缩率下仍能保留关键语义信息。

### 2. 逆向动力学模型（IDM）：自动标注 1100 万小时数据

人工标注昂贵且无法规模化。FDM-1 先在 4 万小时的人工标注数据上训练了一个"逆向动力学模型"——它能从视频帧中反推出用户执行了什么操作（按了哪个键、鼠标移到了哪里）。

核心思路很直觉：如果屏幕上突然出现一个"K"，大概率就是按了 K 键。更复杂的操作（如 Cmd+C 后跟 Cmd+V）则需要几分钟的历史上下文来推断。

该 IDM 采用**掩码扩散架构**，是非因果的——它能同时看到前后帧来做判断，比因果模型更准确、更省数据。

### 3. 前向动力学模型（FDM）：直接在视频上预测下一步操作

与 VLM 不同，FDM 直接在视频和操作 token 上运行——**没有思维链推理、没有 BPE 编码、没有工具调用**。这使推理延迟极低，并能建模当前架构无法处理的任务（如滚动页面、3D 建模、游戏操作）。

鼠标操作的 token 化也颇具巧思：团队使用**指数分箱**将鼠标移动量分配到 49 个不均匀区间——小幅高频移动占据更细的箱，大幅低频移动使用更粗的箱。

### 4. 大规模评估基础设施

团队搭建了可并行运行 **8 万台分叉虚拟机**的评估系统，每小时可执行超过 100 万次 rollout。单块 H100 可同时控制 42 台 VM，从屏幕截取到动作执行的端到端延迟仅 **11 毫秒**。

---

## 实际演示能力

- **CAD 建模**：在 Blender 中挤出多边形面来制作齿轮
- **自动驾驶**：仅用不到 1 小时的微调数据，就能用方向键在旧金山街区自主驾驶转弯
- **网站模糊测试**：自主探索 Web 应用的尽可能多的状态，在模拟银行 App 中发现 Bug

---

## 从数据受限到算力受限

FDM-1 最深层的意义在于**范式转移**：计算机操作 AI 过去受限于数据——标注太贵、数据集太小。现在，有了高效视频编码器和自动标注管线，这个领域首次进入了**算力受限**的阶段——和大语言模型走过的路一样。

正如构建 GPT-3 需要互联网规模的文本语料，构建通用计算机操作 Agent 需要互联网规模的视频语料。FDM-1 迈出了这一步。

---

## 团队与展望

Standard Intelligence 是一支位于旧金山的小团队。他们相信通用人工智能将在未来十年内实现，而 FDM-1 为自主、高能力的计算机操作 Agent 缩小了关键差距。

联系方式：team@si.inc

---

*本文基于 Standard Intelligence 团队 2026 年 2 月 23 日发布的技术博客编译整理。*
